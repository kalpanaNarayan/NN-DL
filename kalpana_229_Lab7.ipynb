{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Neural Network & Deep Learning**"
      ],
      "metadata": {
        "id": "0_-4aHGkAqj2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Kalpana N**  \n",
        "**2347229**"
      ],
      "metadata": {
        "id": "_NPsVCOoA1ma"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Import Required Libraries"
      ],
      "metadata": {
        "id": "HXAzV6_bB49P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import string\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "8gz63yLVNyG8"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Dataset Preparation:**\n",
        "- Download the dataset from Kaggle.\n",
        "- Load the dataset and explore the columns to understand the structure.\n",
        "- Concatenate multiple poems into a single text corpus, separating them by\n",
        "newline characters for clarity."
      ],
      "metadata": {
        "id": "EM0t_d0xN9Fh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "data = pd.read_csv('/content/PoetryFoundationData.csv')\n",
        "# Concatenate poems into a single text corpus\n",
        "corpus = \"\\n\".join(data['Poem'].values)\n",
        "\n",
        "# Limit the dataset to an even smaller size for better memory management\n",
        "lines = corpus.split(\"\\n\")[:500]  # Use only the first 500 lines\n",
        "corpus_trimmed = \"\\n\".join(lines)\n",
        "\n",
        "# If you need to restrict by word count, do so here\n",
        "words = corpus_trimmed.split()[:5000]  # Limit to the first 5,000 words\n",
        "corpus_trimmed = \" \".join(words)"
      ],
      "metadata": {
        "id": "LGYjZkB0N9lR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset Preparation and Trimming**\n",
        "\n",
        "Explanation: We start by loading and trimming the dataset. The goal is to reduce the corpus size to avoid memory overload. By taking only the first 500 lines and limiting it further to 5,000 words, we create a smaller and more manageable text corpus.\n",
        "\n",
        "\n",
        "**Purpose:**\n",
        "\n",
        "This aggressively reduces the size of the dataset to ensure that we don’t run out of memory during preprocessing or model training."
      ],
      "metadata": {
        "id": "lAtyGcnFOKnD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **Data Preprocessing:**\n",
        "- Convert the text to lowercase and remove special characters or\n",
        "punctuation if necessary.\n",
        "- Tokenize the text (e.g., convert each word to a unique integer).\n",
        "- Use a sliding window to create sequences of words for the LSTM model.\n",
        "\n",
        "For example, if n=5, create sequences of 5 words with the 6th word as the\n",
        "target.\n",
        "- Pad the sequences so that they all have the same length."
      ],
      "metadata": {
        "id": "62kyVWGCOPUj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Tokenize and create sequences with reduced max length for memory efficiency\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([corpus_trimmed])\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Create sequences with a shorter max length\n",
        "max_sequence_len = 10  # Shorter max sequence length to reduce padding\n",
        "input_sequences = []\n",
        "\n",
        "# Generate tokenized sequences with the trimmed dataset\n",
        "for line in corpus_trimmed.split(\"\\n\"):\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(token_list)):\n",
        "        n_gram_sequence = token_list[:i+1]\n",
        "        input_sequences.append(n_gram_sequence)\n",
        "\n",
        "# Pad sequences with reduced padding length\n",
        "input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre')\n",
        "\n",
        "# Separate predictors and labels\n",
        "predictors, label = input_sequences[:,:-1], input_sequences[:,-1]\n",
        "label = to_categorical(label, num_classes=total_words)"
      ],
      "metadata": {
        "id": "Wg-eqvXSOLWz"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenization and Sequence Preparation**\n",
        "\n",
        "Explanation: Tokenization is essential to convert words into numerical values for model processing. We create sequences from the text, using a sliding window technique to capture small chunks of text.\n",
        "\n",
        "\n",
        "**Purpose:**\n",
        "\n",
        "This step prepares data for the LSTM model by creating consistent sequences that capture patterns in the text, and padding ensures that all sequences have the same shape for efficient processing.\n",
        "\n"
      ],
      "metadata": {
        "id": "f-G6tYY8OVSt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. **LSTM Model Development:**\n",
        "- Define an LSTM model with the following structure:\n",
        "1. An embedding layer with an appropriate input dimension (based\n",
        "on vocabulary size) and output dimension (e.g., 100).\n",
        "2. One or two LSTM layers with 100 units each.\n",
        "3. A dropout layer with a rate of 0.2 to prevent overfitting.\n",
        "4. A dense output layer with softmax activation for word prediction."
      ],
      "metadata": {
        "id": "BamN81hrOZKr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "\n",
        "# Define and compile the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 50, input_length=max_sequence_len-1))  # Reduced embedding size\n",
        "model.add(LSTM(50, return_sequences=True))  # Smaller LSTM layer to reduce memory\n",
        "model.add(Dropout(0.1))  # Lower dropout rate\n",
        "model.add(LSTM(50))\n",
        "model.add(Dense(total_words, activation='softmax'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMgNlzgOOWBF",
        "outputId": "daa84429-38a7-4822-ae5d-0ab66013fa16"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Architecture**\n",
        "\n",
        "\n",
        "Explanation: We define an LSTM model with a simple structure, optimized for memory efficiency. The architecture includes an embedding layer, two LSTM layers, a dropout layer, and a dense output layer.\n",
        "\n",
        "\n",
        "**Purpose:**\n",
        "\n",
        "This simplified model architecture is designed to capture patterns in text while remaining memory-efficient."
      ],
      "metadata": {
        "id": "wvgHHf-SOqeb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. **Training:**\n",
        "- Compile the model with categorical cross-entropy as the loss function\n",
        "and accuracy as the metric.\n",
        "- Train the model on the sequences for 10-20 epochs (or until it achieves\n",
        "satisfactory performance)."
      ],
      "metadata": {
        "id": "n5HHB4f6OtxM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "history = model.fit(predictors, label, epochs=20, verbose=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Jx0dzrpOq_0",
        "outputId": "5e065f77-855c-469c-e431-16a7561e81cd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 19ms/step - accuracy: 0.0531 - loss: 6.9596\n",
            "Epoch 2/20\n",
            "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.0634 - loss: 6.1964\n",
            "Epoch 3/20\n",
            "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.0597 - loss: 6.1336\n",
            "Epoch 4/20\n",
            "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 0.0615 - loss: 6.1374\n",
            "Epoch 5/20\n",
            "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.0611 - loss: 6.0321\n",
            "Epoch 6/20\n",
            "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.0573 - loss: 5.9340\n",
            "Epoch 7/20\n",
            "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - accuracy: 0.0635 - loss: 5.7941\n",
            "Epoch 8/20\n",
            "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 0.0692 - loss: 5.7148\n",
            "Epoch 9/20\n",
            "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.0630 - loss: 5.6862\n",
            "Epoch 10/20\n",
            "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 0.0636 - loss: 5.6342\n",
            "Epoch 11/20\n",
            "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 0.0676 - loss: 5.5940\n",
            "Epoch 12/20\n",
            "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 0.0638 - loss: 5.5891\n",
            "Epoch 13/20\n",
            "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.0722 - loss: 5.4977\n",
            "Epoch 14/20\n",
            "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.0753 - loss: 5.4512\n",
            "Epoch 15/20\n",
            "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.0885 - loss: 5.3550\n",
            "Epoch 16/20\n",
            "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.0839 - loss: 5.3557\n",
            "Epoch 17/20\n",
            "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 0.0836 - loss: 5.2907\n",
            "Epoch 18/20\n",
            "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.0806 - loss: 5.1591\n",
            "Epoch 19/20\n",
            "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.0884 - loss: 5.1087\n",
            "Epoch 20/20\n",
            "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 24ms/step - accuracy: 0.0857 - loss: 5.0705\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Compilation and Training**\n",
        "\n",
        "Explanation: The model is compiled with categorical_crossentropy as the loss function and adam optimizer, and is trained in small batches to save memory.\n",
        "\n",
        "\n",
        "**Purpose:**\n",
        "\n",
        "Compiling and training the model in small batches allows the model to learn patterns in the text data without exhausting available memory.\n",
        "\n"
      ],
      "metadata": {
        "id": "4w6xiQZtO0xN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. **Text Generation:**\n",
        "- After training, write a function to generate new poetry lines:\n",
        "1. Start with a seed text (e.g., a short phrase).\n",
        "2. Predict the next word, append it to the seed text, and use this\n",
        "updated text to predict the following word.\n",
        "3. Repeat this process for a specified number of words or lines.\n",
        "- Generate multiple lines of poetry using different starting phrases."
      ],
      "metadata": {
        "id": "QyvsCphmO1xP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_poetry(seed_text, next_words=20):\n",
        "    for _ in range(next_words):\n",
        "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "        predicted = model.predict(token_list, verbose=0)\n",
        "        predicted_word_index = np.argmax(predicted, axis=-1)[0]\n",
        "        output_word = tokenizer.index_word[predicted_word_index]\n",
        "        seed_text += \" \" + output_word\n",
        "    return seed_text\n"
      ],
      "metadata": {
        "id": "KYJ49NzXO6Lx"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Text Generation**\n",
        "\n",
        "Explanation: After training, we use the model to generate new lines of poetry. We start with a seed text (a short phrase) and predict the next word repeatedly to form a sequence.\n",
        "\n",
        "**Purpose:**\n",
        "\n",
        "This function generates poetry by iteratively building on a seed phrase, using the learned patterns to predict and add each new word in sequence."
      ],
      "metadata": {
        "id": "xr0g5dyiO_oi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. **Evaluation and Experimentation:**\n",
        "- Experiment with different LSTM layer sizes, dropout rates, and sequence\n",
        "lengths to observe their effects on generated text quality.\n",
        "- Try adding additional LSTM layers and tuning hyperparameters to improve\n",
        "the creativity or fluency of generated poetry."
      ],
      "metadata": {
        "id": "YBLjS-ymPEax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_poetry(\"The sun rises\", next_words=30))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zld6fD15PAbV",
        "outputId": "498f9b4a-ab39-4bb2-f213-02dae2709a8c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The sun rises air air air these the mother to the mother to is to beauty in to by beauty in by were beauty of it's has has has has has has to\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Experimentation and Evaluation**\n",
        "\n",
        "Explanation: This step involves experimenting with model parameters (e.g., number of LSTM layers, dropout rate, batch size, etc.) to understand their impact on text generation quality and model performance. After training and tuning, the model is evaluated based on the fluency, coherence, and creativity of the generated poetry.\n",
        "\n",
        "**Purpose:**\n",
        "\n",
        "Experimentation and evaluation allow you to fine-tune the model and gauge how different configurations impact the quality of generated poetry."
      ],
      "metadata": {
        "id": "ohgmNeK0PMRO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Through the dataset trimming, efficient preprocessing, and carefully structured LSTM architecture, the model successfully generates poetry without exhausting memory. Each part of the code is tailored to maximize learning while managing memory, making it suitable for limited-resource environments. The final poetry generation step produces creative text sequences, and experimentation helps refine the model to improve output quality. This approach provides a balanced framework for handling text generation in constrained computational setups, achieving the objectives of poetic structure and stylistic resemblance."
      ],
      "metadata": {
        "id": "UJXgtZWLPS3h"
      }
    }
  ]
}